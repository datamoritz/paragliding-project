<!DOCTYPE html>
<html lang="en">
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Decision Trees - Paragliding ML Project</title>
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>
  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="index.html">Paragliding ML</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarNavDropdown">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="introduction.html">Introduction</a></li>
          <li class="nav-item"><a class="nav-link" href="dataprep.html">Data Prep / EDA</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">ML Models</a>
            <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
              <li><a class="dropdown-item" href="clustering.html">Clustering</a></li>
              <li><a class="dropdown-item" href="pca.html">PCA</a></li>
              <li><a class="dropdown-item" href="naivebayes.html">Naive Bayes</a></li>
              <li><a class="dropdown-item" href="decisiontrees.html">Decision Trees</a></li>
              <li><a class="dropdown-item" href="svm.html">SVMs</a></li>
              <li><a class="dropdown-item" href="regression.html">Regression</a></li>
              <li><a class="dropdown-item" href="nn.html">Neural Networks</a></li>
            </ul>
          </li>
          <li class="nav-item"><a class="nav-link" href="conclusions.html">Conclusions</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <div class="container mt-5">
  <div class="col-lg-8 mx-auto">
    <h1 class="fw-bold">Decision Trees</h1>
      <!-- Left-aligned inline navigation -->
    
      <div class="mb-2">
        <a href="#overview" class="me-3 text-primary fw-semibold text-decoration-none">Overview</a>
        <a href="#data" class="me-3 text-primary fw-semibold text-decoration-none">Data</a>
        <a href="#code" class="me-3 text-primary fw-semibold text-decoration-none">Code</a>
        <a href="#results" class="me-3 text-primary fw-semibold text-decoration-none">Results</a>
        <a href="#conclusions" class="text-primary fw-semibold text-decoration-none">Conclusions</a>
      </div>
      
      <hr>

<figure class="d-flex flex-column align-items-center my-4">

  <img src="images/DT1.png"
       alt="Illustration of Decision Tree classification concept"
       class="img-fluid rounded shadow-sm"
       style="max-width: 60%; height: auto;">

  <figcaption class="mt-3 text-muted"
              style="max-width: 500px; line-height: 1.6; font-size: 0.9rem;">
    Illustration of the Na√Øve Bayes principle: features (shapes) influence 
    the probability of belonging to different classes using Bayes‚Äô theorem.  
    Image inspired by an explanation from 
    <a href="#ref1"><sup>[1]</sup></a>.
  </figcaption>

</figure>



    
<p>
Decision Trees (DTs) are supervised learning methods that can be used for classification and regression. 
They split data using simple decision rules to classify examples or make predictions. They work by repeatedly 
dividing the dataset into smaller, more homogenous groups based on feature values. The visual representation 
of a decision tree consists of:
</p>

<ul>
  <li>Root Node: Starting point representing the whole dataset.</li>
  <li>Branches: Lines connecting nodes showing the flow from one decision to another.</li>
  <li>Internal Nodes: Points where decisions are made based on data features.</li>
  <li>Leaf Nodes: End points of the tree where the final decision or prediction is made (class label is assigned).</li>
</ul>

<p>
Decision Trees choose splits by measuring how pure the resulting groups become. A pure node contains mostly (or entirely) 
one class. Pure can be defined using entropy (measure of disorder) or the GINI impurity. In practice, Gini and Entropy often 
produce similar trees. Synonymous for minimizing impurity is maximizing the information gain (IG) with each split.
</p>

<ul>
<li>
<h5>GINI Impurity</h5>

<p class="text-center">
  \[
    G = 1 - \sum_i p_i^2
  \]
</p>

<p>Gini measures class mixture and is relatively simple to compute.</p>
</li>

<li>
<h5>Entropy</h5>

<p class="text-center">
  \[
    H(p) = -\sum_i p_i \log_2(p_i)
  \]
</p>

<ul>
  <li>Entropy = 0 ‚Üí perfectly pure</li>
  <li>Entropy = high ‚Üí mixed classes</li>
</ul>
</li>
  
<h5>Information Gain</h5>

<p class="text-center">
  \[
    IG = H(\text{parent}) - \sum_j \frac{N_j}{N} H(\text{child}_j)
  \]
</p>

<p>
The best split is the one with maximum information gain.
</p>
  
</li>
</ul>


  
<figure class="d-flex flex-column align-items-center my-4">

  <img src="images/DT2.png"
       alt="Illustration of Decision Tree classification concept"
       class="img-fluid rounded shadow-sm"
       style="max-width: 80%; height: auto;">

  <figcaption class="mt-3 text-muted"
              style="max-width: 500px; line-height: 1.6; font-size: 0.9rem;">
    Illustration of the Na√Øve Bayes principle: features (shapes) influence 
    the probability of belonging to different classes using Bayes‚Äô theorem.  
    Image inspired by an explanation from 
    <a href="#ref1"><sup>[1]</sup></a>.
  </figcaption>

</figure>
  
  
<h5>Example:</h5>

<p>
In the example in the figure above the root node contains 30 samples. 16 blue and 14 orange. The class probabilities are:
</p>

<p class="text-center">
  \[
  P(\text{blue}) = \frac{16}{30} = 0.53
  \]
</p>

<p class="text-center">
  \[
  P(\text{orange}) = \frac{14}{30} = 0.47
  \]
</p>

<p>
And the entropy of the root node is:
</p>

<p class="text-center">
  \[
  H_{\text{parent}} \approx -0.53\log_2(0.53) - 0.47\log_2(0.47) \approx 0.996
  \]
</p>

<p>
The split is performed using the feature balance. After the split the left child has 13 samples (12 blue, 1 orange)
</p>
  

<p class="text-center">
  \[
  P(\text{blue}) = 12/13 = 0.92,\quad P(\text{orange}) = 1/13 = 0.08
  \]
</p>

<p class="text-center">
  \[
  H_L \approx -0.92\log_2(0.92) - 0.08\log_2(0.08) \approx 0.39
  \]
</p>

<p>
The right child has 17 samples (4 blue and 13 orange).
</p>

<p class="text-center">
  \[
  P(\text{blue}) = 4/17 = 0.24, \quad P(\text{orange}) = 13/17 = 0.76
  \]
</p>

<p class="text-center">
  \[
  H_R \approx -0.24\log_2(0.24) - 0.76\log_2(0.76) \approx 0.78
  \]
</p>

<p>
Using these value we can calculate the weighted entropy after the split, and with that obtain a value for the information gain:
</p>

<p class="text-center">
  \[
  H_{\text{after}} = \frac{13}{30}H_L + \frac{17}{30}H_R 
  = \frac{13}{30}(0.39) + \frac{17}{30}(0.78) \approx 0.615
  \]
</p>

<p class="text-center">
  \[
  IG = H_{\text{parent}} - H_{\text{after}} = 0.996 - 0.615 = 0.381
  \]
</p>

<p>
A positive (and relatively large) information gain means the split greatly increases purity and is therefore chosen by 
the Decision Tree.
</p>



<p>
Decision trees can grow indefinitely, because continuous features allow infinitely many split points. They can keep splitting 
until every sample is isolated (the tree is oerfectly tuned to the training data which causes overfitting). To prevent this 
hyperparameters are used, e.g. max_depth.
</p>

<h5>Random Forest</h5>

<p>
Random Forests are a widely used ensemble learning method that builds many independent decision trees and combines their 
predictions. This enhances accuracy and predictive power. Each tree is trained on a bootstrap sample (random subset) of the 
training data, and at each split only a random subset of features is considered. This injected randomness ensures that the 
trees learn different patterns rather than all overfitting to the same structure. Predictions are made through majority voting 
(for classification) or averaging (for regression). 
Because Random Forests aggregate the outputs of many weak learners, they typically achieve higher accuracy, greater robustness 
against noise, and better generalization than a single Decision Tree. They also provide feature importance scores, making it 
possible to interpret which variables contribute most to the model‚Äôs decisions.
</p>

  <h3>Data</h3>
        <p>
          üîó <a href="https://github.com/datamoritz/paragliding-project/blob/main/data/sample_thermal_dataset.csv"
             target="_blank" class="text-decoration-none text-primary">
            Download sample dataset
          </a>
        </p>
    
        <p>
            Supervised machine learning requires labeled data. Each record must belong to a known category. 
            In this project, the labels are the thermal cluster classes (1‚Äì5) derived from K-Means in the clustering tab. 
            Next, the data must be split into two disjoint sets:
        </p>
    
        <ul>
            <li>Training Set: used to learn the model</li>
            <li>Testing Set: used to evaluate performance on unseen data</li>
        </ul>
    
        <p>
            Keeping them separate (disjoint) prevents overfitting and allows to realistic test and evaluate the accurary 
            of the model on unseen data. In this project stratification was used to preserve the relative class proportions. 
            Below is a small sample of the dataset used (weather features, and the cluster label):
        </p>

      <figure class="d-flex flex-column align-items-center my-4">

        <img src="images/DT3.png"
             alt="Example of weather features and corresponding thermal cluster labels before model training"
             class="img-fluid rounded shadow-sm"
             style="max-width: 100%; height: auto;">
      
        <figcaption class="mt-3 text-muted"
                    style="max-width: 800px; line-height: 1.6; font-size: 0.9rem;">
          Example of the prepared dataset used for Na√Øve Bayes: nine weather features 
          (e.g., temperature, dewpoint, wind speed, cloud cover, shortwave radiation, 
          boundary-layer height) combined with the target label 
          <code>cluster</code> (1‚Äì5).  
          The image illustrates the transformation from raw weather data into the 
          final supervised learning format used for classification.
        </figcaption>
      
      </figure>


      

      <h3 id="code">Code</h3>
      <p>
        üîó <a href="https://github.com/datamoritz/paragliding-project/blob/main/code/4_Clustering_v1.ipynb">
          Link to code
        </a>
      </p>


    
    <h3>Results</h3>

     <figure class="d-flex flex-column align-items-center my-4">
    
      <img src="images/DT4.png"
           alt="Na√Øve Bayes confusion matrix and per-class performance metrics"
           class="img-fluid rounded shadow-sm"
           style="max-width: 100%; height: auto;">
  

       <figcaption class="mt-3 text-muted" 
            style="max-width: 800px; line-height: 1.6; font-size: 0.9rem;">
      Decision Tree evaluation results. Left: Confusion matrix showing how often each thermal cluster (1‚Äì5) 
      is correctly or incorrectly predicted. Right: Feature importance ranking highlighting which weather 
      variables most influence the classification. Dewpoint, shortwave radiation, and windspeed are the 
      dominant predictors, while cloud cover features contribute less. Classes 1 and 5 are recognized more 
      reliably, while mid-range classes (2‚Äì4) show higher overlap in feature space.
    </figcaption>
    
    </figure>


      <figure class="d-flex flex-column align-items-center my-4">
    
      <img src="images/DT5.png"
           alt="Na√Øve Bayes confusion matrix and per-class performance metrics"
           class="img-fluid rounded shadow-sm"
           style="max-width: 100%; height: auto;">
    
  <figcaption class="mt-3 text-muted"
              style="max-width: 800px; line-height: 1.6; font-size: 0.95rem;">
    Random Forest evaluation results. Left: Confusion matrix showing how often each thermal cluster (1‚Äì5)
    is correctly or incorrectly predicted. Right: Feature importance chart indicating which weather variables
    contributed most to the model‚Äôs decisions. The Random Forest achieves higher accuracy than Na√Øve Bayes,
    with Classes 1 and 5 recognized more reliably, while Classes 2‚Äì4 remain harder to separate due to
    overlapping weather characteristics.
  </figcaption>
    
    </figure>
<p>
    The <strong>Decision Tree</strong> achieved an accuracy of 40.2%, showing clear limitations in 
    separating the five thermal-quality classes. The confusion matrix highlights large misclassifications 
    across the mid-range classes (2‚Äì4), which the model often predicts as either class 1 or class 5. 
    Feature importances indicate that only a few variables‚Äî<em>dewpoint</em>, <em>shortwave radiation</em>, 
    <em>windspeed</em>, and <em>boundary layer height</em>‚Äîdominate the decision process, suggesting that the 
    tree relies heavily on meteorological extremes rather than subtle patterns.
</p>

<p>
    The <strong>Random Forest (Tuned)</strong> performed substantially better, reaching 
    50.3% accuracy. This was achieved after a parameter optimization using CV Search. Misclassifications are still present, but the forest captures more 
    structure in the data, especially for classes 1, 3, and 5. Feature importances are more evenly 
    distributed compared to the Decision Tree, with <em>dewpoint</em>, <em>shortwave radiation</em>, 
    <em>temperature</em>, and <em>boundary layer height</em> consistently ranking as the most influential 
    predictors. This indicates that ensemble averaging helps stabilize decisions and extract more meaningful 
    signals from the overlapping feature space.
</p>


    

  <h3>Conclusion</h3>
    
<p>
    The results show that while weather variables contain useful information for predicting 
    thermal-quality classes, the separation between classes remains challenging due to overlapping 
    atmospheric conditions. The Random Forest notably improves performance over a single Decision Tree, and also outperforming Naive Bayes. 
     However, even 
    with tuning, accuracy plateaus around 50%, suggesting that weather alone cannot fully 
    discriminate the five cluster classes. Incorporating additional flight-level features 
    (e.g., time of day, terrain characteristics, lapse rate, vertical velocity forecasts) or using more detailed weather features may improve predictive accuracy further.
</p>

<p>
    The feature importance rankings highlight why variables such dewpoint and 
    shortwave radiation dominate the models‚Äô decisions. Dewpoint is closely tied to 
    atmospheric moisture and the stability of rising air. Lower dewpoint values generally indicate 
    drier, more unstable conditions that enable stronger thermal formation, while higher dewpoints are 
    associated with moisture, cloud development, and reduced thermal strength. Similarly, 
    shortwave radiation directly captures the amount of incoming solar energy heating 
    the ground. Stronger surface heating creates larger temperature gradients and more vigorous 
    convective lift, making it one of the primary drivers of thermal quality on any given day.
</p>

    
      <hr>
      <h6 class="fw-semibold mt-4">References</h6>
      <ol class="small text-muted" style="line-height: 1.8;">

      <li id="ref1">
        <a href="https://databasecamp.de/en/ml/naive-bayes-algorithm#:~:text=The%20Naive%20Bayes%20Algorithm%20is%20a%20classification,occurrence%20of%20another%20feature%20within%20the%20class."
           target="_blank"
           class="text-decoration-none text-primary">
           ‚ÄúNaive Bayes Algorithm.‚Äù DatabaseCamp.de
        </a>
        ‚Äî Accessed October 2025.
      </li>
        
        <li id="ref2">
          <a href="https://miro.medium.com/v2/resize:fit:1576/1*vAtQZbROuTdp36aQQ8cqBA.png"
             target="_blank" class="text-decoration-none text-primary">
             ‚ÄúComparison of Euclidean, Manhattan, and Cosine Similarity Metrics.‚Äù <em>Medium</em>.
          </a>
          Accessed October 2025.
        </li>

      </ol>
    </div>
