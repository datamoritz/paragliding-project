<!DOCTYPE html>
<html lang="en">
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Boosting - Paragliding ML Project</title>
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>
  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="index.html">Paragliding ML</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarNavDropdown">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="introduction.html">Introduction</a></li>
          <li class="nav-item"><a class="nav-link" href="dataprep.html">Data Prep / EDA</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">ML Models</a>
            <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
              <li><a class="dropdown-item" href="clustering.html">Clustering</a></li>
              <li><a class="dropdown-item" href="pca.html">PCA</a></li>
              <li><a class="dropdown-item" href="naivebayes.html">Naive Bayes</a></li>
              <li><a class="dropdown-item" href="decisiontrees.html">Decision Trees</a></li>
              <li><a class="dropdown-item" href="boosting.html">Boosting</a></li>
            </ul>
          </li>
          <li class="nav-item"><a class="nav-link" href="conclusions.html">Conclusions</a></li>
        </ul>
      </div>
    </div>
  </nav>




  

    <div class="container mt-5">
    <div class="col-lg-8 mx-auto">
    <h1 class="fw-bold">Ensemble & Boosting Methods</h1>
      
      <!-- Left-aligned inline navigation -->
      <div class="mb-2">
        <a href="#overview" class="me-3 text-primary fw-semibold text-decoration-none">Overview</a>
        <a href="#data" class="me-3 text-primary fw-semibold text-decoration-none">Data</a>
        <a href="#code" class="me-3 text-primary fw-semibold text-decoration-none">Code</a>
        <a href="#results" class="me-3 text-primary fw-semibold text-decoration-none">Results</a>
        <a href="#conclusions" class="text-primary fw-semibold text-decoration-none">Conclusions</a>
      </div>
      
      <hr>

    <h3>Overview</h3>

    <figure class="d-flex flex-column align-items-center my-4">
    
      <img src="images/Boosting1.png"
           alt="Illustration of boosting with reweighted datasets"
           class="img-fluid rounded shadow-sm"
           style="max-width: 60%; height: auto;">
    
      <figcaption class="mt-3 text-muted"
                  style="max-width: 500px; line-height: 1.6; font-size: 0.9rem;">
        Visual explanation of the boosting process: each weak learner is trained 
        sequentially, with sample weights adjusted after every iteration to focus on 
        hard-to-classify points. By combining these weak learners, boosting forms a 
        strong final model. <a href="#ref1"><sup>[1]</sup></a>
      </figcaption>
    
    </figure>


      
    <p>
      Ensemble methods combine multiple models to create a more accurate and robust predictor than any single model alone. 
      Boosting is a specific type of ensemble technique where models are built sequentially. Here, each new model focuses 
      on correcting the errors made by earlier ones. The individual building blocks of boosting are so called weak learners. 
      These are typically very simple models such as decision stumps, that perform only slightly better than random guessing. 
      Repeatedly training weak learners on reweighted data or on residual errors, they are gradually combined and converted 
      into a strong, high-performing model. The key principle is to iteratively learn from mistakes and to give more attention 
      to the examples the model previously struggled with.
    </p>


      
    <h3>How Boosting Works</h3>
    <p>
      Boosting trains models in a strict sequence, where each learner is built to address the errors of the previous one. 
      The typical steps involved include:
    </p>
    
    <ul>
      <li><strong>Initialize Weights:</strong> Start by giving all training samples equal importance.</li>
      <li><strong>Train Sequentially:</strong> Fit the initial weak model, then increase the weights of the samples it misclassified so the next learner focuses on harder cases.</li>
      <li><strong>Iterative Refinement:</strong> Continue training additional weak learners, each time updating sample weights so new models target the current ensemble‚Äôs mistakes.</li>
      <li><strong>Combine Learners:</strong> Produce the final prediction by combining all weak learners, typically using weighted voting where better-performing models receive higher influence.</li>
    </ul>
    
    <p>
      This method effectively minimizes errors by focusing more intensively on difficult cases in the training data. 
      The result is improved accuracy and strong predictive performance. Because each update is kept small and controlled, 
      boosting also helps manage variance, leading to a final model that is both accurate and stable.
    </p>
    
    <h3>Major Boosting Algorithms</h3>
    
    <h4>3.1 AdaBoost</h4>
    <p>
      AdaBoost builds a sequence of weak learners (often decision stumps), each trained on a reweighted dataset. 
      Weights are updated by increasing the importance of misclassified samples so later learners focus more on harder cases. 
      More accurate learners receive higher model weights in the final ensemble. AdaBoost is strong at reducing bias, 
      with some risk of increasing variance if too many rounds are added.
    </p>
    
    <h4>3.2 Gradient Boosting (GBM)</h4>
    <p>

      <figure class="d-flex flex-column align-items-center my-4">

      <img src="images/Boosting2.png"
           alt="Illustration of iterative optimization in boosting via gradient descent"
           class="img-fluid rounded shadow-sm"
           style="max-width: 60%; height: auto;">
    
      <figcaption class="mt-3 text-muted"
                  style="max-width: 500px; line-height: 1.6; font-size: 0.9rem;">
        Illustration of iterative optimization: each learning step gradually reduces 
        the loss function until reaching a local minimum. This visual concept is central 
        to gradient boosting, where each learner fits the gradient of the current loss. 
        <a href="#ref2"><sup>[2]</sup></a>
      </figcaption>

        
    </figure>
      
      Gradient Boosting uses residual-based optimization: each new tree is trained to predict the residuals (errors) 
      of the current ensemble. Boosting is framed as a gradient descent problem on a chosen loss function, allowing flexible 
      use with regression and classification models. GBM is highly flexible and accurate, but can be slow to train and prone 
      to overfitting without careful tuning.
    </p>
    
    <h4>3.3 XGBoost</h4>
    <p>
      XGBoost (Extreme Gradient Boosting) is an optimized, distributed gradient boosting algorithm and often the most effective 
      choice in practice. Building on GBM, it incorporates second-order gradient information and system-level optimizations for speed. 
      It includes L1/L2 regularization, column subsampling, multithreading, and efficient tree construction to reduce overfitting and 
      improve performance. XGBoost also learns optimal directions for missing data during splits, making it robust to incomplete datasets.
    </p>

      
    
        
        <h3>Data</h3>
        <p>
          üîó <a href="https://github.com/datamoritz/paragliding-project/blob/main/data/sample_thermal_dataset.csv"
             target="_blank" class="text-decoration-none text-primary">
            Download sample dataset
          </a>
        </p>
    
        <p>
            Supervised machine learning requires labeled data. Each record must belong to a known category. 
            In this project, the labels are the thermal cluster classes (1‚Äì5) derived from K-Means in the clustering tab. 
            Next, the data must be split into two disjoint sets:
        </p>
    
        <ul>
            <li>Training Set: used to learn the model</li>
            <li>Testing Set: used to evaluate performance on unseen data</li>
        </ul>
    
        <p>
            Keeping them separate (disjoint) prevents overfitting and allows to realistic test and evaluate the accurary 
            of the model on unseen data. In this project stratification was used to preserve the relative class proportions. 
            Below is a small sample of the dataset used (weather features, and the cluster label):
        </p>

      <figure class="d-flex flex-column align-items-center my-4">

        <img src="images/NB2.png"
             alt="Example of weather features and corresponding thermal cluster labels before model training"
             class="img-fluid rounded shadow-sm"
             style="max-width: 90%; height: auto;">
      
        <figcaption class="mt-3 text-muted"
                    style="max-width: 800px; line-height: 1.6; font-size: 0.9rem;">
          Example of the prepared dataset used for Na√Øve Bayes: nine weather features 
          (e.g., temperature, dewpoint, wind speed, cloud cover, shortwave radiation, 
          boundary-layer height) combined with the target label 
          <code>cluster</code> (1‚Äì5).  
          The image illustrates the transformation from raw weather data into the 
          final supervised learning format used for classification.
        </figcaption>
      
      </figure>


      

      <h3 id="code">Code</h3>
      <p>
        üîó <a href="https://github.com/datamoritz/paragliding-project/blob/main/code/">
          Link to code
        </a>
      </p>


      
    <h3>Results</h3>
      
    <figure class="d-flex flex-column align-items-center my-4">
    
      <img src="images/XGBoost_CM.png"
           alt="XGBoost confusion matrix"
           class="img-fluid rounded shadow-sm"
           style="max-width: 100%; height: auto;">
    
      <figcaption class="mt-3 text-muted"
                  style="max-width: 800px; line-height: 1.6; font-size: 0.9rem;">
        XGBoost evaluation results.  
        The confusion matrix shows that Cluster&nbsp;0 and Cluster&nbsp;4 are predicted most reliably, 
        while mid-range clusters (1‚Äì3) exhibit substantial confusion due to overlapping thermal features.  
        Overall accuracy is 48.75%, highlighting that nonlinear tree-based modeling captures some 
        cluster structure but still struggles to clearly separate intermediate classes.
      </figcaption>
    
    </figure>

      
<p>
    The final XGBoost model achieved an accuracy of <strong>~48.75%</strong>, clearly above random guessing (20% with five classes) but still indicating substantial overlap between the thermal-quality clusters. 
    The confusion matrix shows that the model predicts the extreme classes (0 and 4) more reliably, while the intermediate classes (1‚Äì3) are frequently confused. 
    Class&nbsp;2 and Class&nbsp;3 in particular show very weak separability, suggesting that the underlying weather patterns for those clusters are highly similar.
</p>

<p>
    The hyperparameter sweep (max_depth √ó learning_rate) confirms this limitation: 
    across all tested configurations, accuracy remains tightly clustered between <strong>0.43 and 0.49</strong>. 
    Even with deeper trees (depth 5‚Äì6), which generally performed best, the overall improvement is modest. 
    This plateau suggests that model performance is constrained more by the data structure than by tuning‚Äîi.e., the available weather features do not cleanly separate all five classes.
</p>

<p>
    Feature importance values shed further light on the model‚Äôs behavior. 
    Dew point temperature, shortwave radiation, and boundary layer height emerge as the strongest predictors, followed by wind speed and cloud cover variables. 
    These factors align well with physical expectations for thermal strength. 
    Nevertheless, the relatively even distribution of importance across many features indicates that no single variable dominates the classification, 
    reinforcing the idea that the thermal clusters are not sharply defined in weather-feature space.
</p>

<figure class="d-flex flex-column align-items-center my-4">

  <img src="images/XGBoost_HM.png"
       alt="XGBoost accuracy heatmap for depth and learning rate"
       class="img-fluid rounded shadow-sm"
       style="max-width: 100%; height: auto;">

  <figcaption class="mt-3 text-muted"
              style="max-width: 800px; line-height: 1.6; font-size: 0.9rem;">
    Accuracy of XGBoost models across combinations of tree depth and learning rate.  
    The best-performing region lies around max_depth 5‚Äì6 with learning rates between 0.03 
    and 0.10 (~0.49 accuracy), while shallower trees remain noticeably weaker.
  </figcaption>

</figure>

      <figure class="d-flex flex-column align-items-center my-4">

  <img src="images/XGBoost_FI.png"
       alt="XGBoost feature importance bar plot"
       class="img-fluid rounded shadow-sm"
       style="max-width: 100%; height: auto;">

  <figcaption class="mt-3 text-muted"
              style="max-width: 800px; line-height: 1.6; font-size: 0.9rem;">
    XGBoost feature importance values.  
    Dewpoint, shortwave radiation, and boundary layer height contribute most strongly 
    to cluster predictions, while wind- and cloud-related variables also provide 
    substantial signal for thermal classification.
  </figcaption>

</figure>

    <h3>Conclusion</h3>
<p>
  XGBoost was able to uncover the same meaningful structure in the weather features as Nauves Bayes and Random Forest did. It produced solid performance, 
  particularly for the most distinct thermal clusters. Boosting improved classification by iteratively correcting 
  errors, but its accuracy remained slightly below that of the Random Forest. The analysis showed that XGBoost is highly sensitive to hyperparameters such 
  as tree depth and learning rate‚Äîmoderate settings offered the best balance between accuracy and overfitting. 
  With more data, richer data or improved cluster labels, XGBoost could likely match or surpass the Random Forest.
</p>


      

      <hr>
      <h6 class="fw-semibold mt-4">References</h6>
      <ol class="small text-muted" style="line-height: 1.8;">
      
        <li id="ref1">
          <a href="https://mathchi.medium.com/weak-learners-strong-learners-for-machine-learning-e73e32f86ebd"
             target="_blank"
             class="text-decoration-none text-primary">
             ‚ÄúWeak Learners ‚Üí Strong Learners.‚Äù Medium.
          </a>
          ‚Äî Accessed November 2025.
        </li>
      
        <li id="ref2">
          <a href="https://bradleyboehmke.github.io/HOML/gbm.html"
             target="_blank"
             class="text-decoration-none text-primary">
             ‚ÄúGradient Boosting Machines.‚Äù Hands-On Machine Learning (Boehmke & Greenwell).
          </a>
          ‚Äî Accessed November 2025.
        </li>
      
      </ol>
    </div>
