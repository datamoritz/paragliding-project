<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PCA - Paragliding ML Project</title>
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>
  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    <div class="container-fluid">
      <a class="navbar-brand" href="index.html">Paragliding ML</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarNavDropdown">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="introduction.html">Introduction</a></li>
          <li class="nav-item"><a class="nav-link" href="dataprep.html">Data Prep / EDA</a></li>
          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">ML Models</a>
            <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
              <li><a class="dropdown-item" href="clustering.html">Clustering</a></li>
              <li><a class="dropdown-item" href="pca.html">PCA</a></li>
              <li><a class="dropdown-item" href="naivebayes.html">Naive Bayes</a></li>
              <li><a class="dropdown-item" href="decisiontrees.html">Decision Trees</a></li>
              <li><a class="dropdown-item" href="svm.html">SVMs</a></li>
              <li><a class="dropdown-item" href="regression.html">Regression</a></li>
              <li><a class="dropdown-item" href="nn.html">Neural Networks</a></li>
            </ul>
          </li>
          <li class="nav-item"><a class="nav-link" href="conclusions.html">Conclusions</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <div class="container mt-5">
    <h1 class="fw-bold">PCA (Principal Component Analysis)</h1>
    <hr>
    <h3>Overview</h3>
    <p>

      For every thermal, i.e. row in the thermal datafile, there is a large amount of weather variables (wind, temperature, cloud base, etc.). This can slow any machine learning algorithm down and is difficult to visualize. The goal of this section is to introduce a method to significantly reduce the dimensions of our thermals dataset.
Principle Component Analysis (PCA) is a powerful dimensionality reduction technique to reduce the number of features in a dataset while retaining most of the information. This is achieved by looking at their correlation, i.e. if they are highly similar. However, instead of just removing a column, PCA works in a more sophisticated which is described in the following.
    
    
    </p>


    <h4>Curse of Dimensionality</h4>
    <p>
    High-dimensional data presents several challenges known as the curse of dimensionality:
•	Sparsity: As dimensions grow, data points become more spread out, making patterns harder to detect
•	Computational cost: Algorithms become exponentially slower and more memory-intensive
•	Distance distortion: In high dimensions, distances between points become less meaningful as everything appears equally far apart
•	Overfitting risk: Models may learn noise rather than structure
PCA helps mitigate these issues by compressing the dataset into fewer, more informative dimensions. The resulting data is denser, faster to process, and easier to visualize or feed into machine learning models.

      
    </p>
    <h4>Eigenvectors and -values</h4>
    <p>
PCA identifies combinations of features that explain the most variance in the dataset. Each principal component is a weighted linear combination of the original variables, chosen so that it captures as much of the underlying structure as possible. At the heart of PCA are eigenvalues and eigenvectors, which describe the directions and strengths of variation in the data.
•	An eigenvector represents the direction of maximum variance — it defines one of the new axes (principal components) along which the data is spread.
•	The corresponding eigenvalue measures the amount of variance captured along that direction.
Larger eigenvalues indicate directions that explain more of the dataset’s structure. By ranking eigenvalues from largest to smallest, PCA selects the top few eigenvectors (principal components) that retain the majority of the data’s information.

      
    </p>

    <div class="d-flex justify-content-center align-items-start mt-4 gap-3">
  <figure class="text-center" style="width: 48%;">
    <img src="https://www.machinelearningplus.com/wp-content/uploads/2019/11/PCA_Projections.gif"
         alt="PCA projection 2D" class="img-fluid rounded shadow-sm">
    <figcaption class="mt-2 text-muted small">
      Projection of 2D data (x, y) onto first principal component (black line); red lines show orthogonal projections capturing maximum variance.
    </figcaption>
  </figure>

  <figure class="text-center" style="width: 48%;">
    <img src="https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2023/01/PCA_original.png?w=596&ssl=1"
         alt="PCA components in 2D" class="img-fluid rounded shadow-sm">
    <figcaption class="mt-2 text-muted small">
      Principal components in 2D: V1 captures maximum variance along the data spread; V2 is orthogonal and captures remaining variance.
    </figcaption>
  </figure>
</div>

    <h4>Dimensionality Reduction</h4>
    <p>
Dimensionality reduction is the process of decreasing the number of input features in a dataset while keeping as much relevant information as possible.For example, of 50 somewhat correlated variables, PCA might reduce them to 3–5 principal components that summarize the original structure effectively. 
Reducing dimensionality is crucial for:
•	Simplifying analysis: fewer variables make statistical models easier to interpret.
•	Improving visualization: data in 2D or 3D can be plotted, revealing clusters or trends.
•	Reducing noise: small, uninformative variations are filtered out.
•	Preventing overfitting: fewer redundant features mean models generalize better.
However, a downside of PCA is that the resulting principal components no longer correspond directly to the original variables. This sacrifices interpretability. Also, the underlying relationships are nonlinear, PCA can still run, but it will not capture nonlinear structure effectively.

      
    </p>

    
    
    <h3>Data</h3>
    <p>PCA requires numerical, continuous data where each column represents a variable (feature) and each row represents an observation (sample). Data must be standardized or normalized, since PCA is sensitive to scale differences. Categorical or text variables need to be encoded numerically (e.g., one-hot encoding) if they shall be included. Moreover, the dataset should have linear relationships between variables for meaningful variance capture.</p>

    <h3>Code</h3>
    <p>Link to the code repository or notebook. Note the programming language and core packages used. (Do not paste the code here.)</p>

    <h3>Results</h3>
    <p>Discussion with figures: parameters tried, visualizations, and interpretation of results with respect to the project questions.</p>
  </div>
